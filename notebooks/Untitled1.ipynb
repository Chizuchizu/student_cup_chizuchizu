{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (0.6.2)\n",
      "Requirement already satisfied: numpy in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from pytorch-pretrained-bert) (1.18.5)\n",
      "Requirement already satisfied: regex in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from pytorch-pretrained-bert) (2020.7.14)\n",
      "Requirement already satisfied: tqdm in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from pytorch-pretrained-bert) (4.48.2)\n",
      "Requirement already satisfied: requests in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from pytorch-pretrained-bert) (2.24.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from pytorch-pretrained-bert) (1.6.0)\n",
      "Requirement already satisfied: boto3 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from pytorch-pretrained-bert) (1.14.36)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: future in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.18.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.36 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from boto3->pytorch-pretrained-bert) (1.17.36)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from botocore<1.18.0,>=1.17.36->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from botocore<1.18.0,>=1.17.36->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.36->boto3->pytorch-pretrained-bert) (1.15.0)\n",
      "Collecting https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip\n",
      "  Downloading https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip\n",
      "\u001b[K     \\ 7.3 kB 8.3 MB/s\n",
      "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from PyTorchHelperBot==0.0.4) (1.6.0)\n",
      "Requirement already satisfied: future in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from torch>=0.4.1->PyTorchHelperBot==0.0.4) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages (from torch>=0.4.1->PyTorchHelperBot==0.0.4) (1.18.5)\n",
      "Building wheels for collected packages: PyTorchHelperBot\n",
      "  Building wheel for PyTorchHelperBot (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyTorchHelperBot: filename=PyTorchHelperBot-0.0.4-py3-none-any.whl size=7192 sha256=8611f11a6396da12a07a01e037422bc232a34de0c63d8efa6cc5390d2276dff4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uwriit5r/wheels/89/ca/eb/88082aef9fc507fdfdbf8baf46969e4c3e7896cc03857b5c6a\n",
      "Successfully built PyTorchHelperBot\n",
      "Installing collected packages: PyTorchHelperBot\n",
      "Successfully installed PyTorchHelperBot-0.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert\n",
    "!pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip\n",
    "import os\n",
    "\n",
    "# This variable is used by helperbot to make the training deterministic\n",
    "os.environ[\"SEED\"] = \"33223\"\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "from helperbot import BaseBot, TriangularLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LightGBM'...\n",
      "remote: Enumerating objects: 38, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 18833 (delta 5), reused 6 (delta 2), pack-reused 18795\u001b[K\n",
      "Receiving objects: 100% (18833/18833), 12.73 MiB | 3.84 MiB/s, done.\n",
      "Resolving deltas: 100% (13746/13746), done.\n",
      "Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'compute'\n",
      "Cloning into '/home/yuma/PycharmProjects/student_cup/notebooks/LightGBM/build/LightGBM/build/LightGBM/build/LightGBM/compute'...\n",
      "remote: Enumerating objects: 21728, done.        \n",
      "remote: Total 21728 (delta 0), reused 0 (delta 0), pack-reused 21728        \n",
      "Receiving objects: 100% (21728/21728), 8.51 MiB | 5.38 MiB/s, done.\n",
      "Resolving deltas: 100% (17565/17565), done.\n",
      "Submodule path 'compute': checked out '36c89134d4013b2e5e45bc55656a18bd6141995a'\n",
      "/home/yuma/PycharmProjects/student_cup/notebooks/LightGBM/build/LightGBM/build/LightGBM/build/LightGBM\n",
      "/home/yuma/PycharmProjects/student_cup/notebooks/LightGBM/build/LightGBM/build/LightGBM/build/LightGBM/build\n",
      "-- The C compiler identification is GNU 7.5.0\n",
      "-- The CXX compiler identification is GNU 7.5.0\n",
      "-- Check for working C compiler: /usr/bin/cc\n",
      "-- Check for working C compiler: /usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- Performing Test MM_PREFETCH\n",
      "-- Performing Test MM_PREFETCH - Success\n",
      "-- Using _mm_prefetch\n",
      "-- Performing Test MM_MALLOC\n",
      "-- Performing Test MM_MALLOC - Success\n",
      "-- Using _mm_malloc\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/yuma/PycharmProjects/student_cup/notebooks/LightGBM/build/LightGBM/build/LightGBM/build/LightGBM/build\n",
      "CMakeCache.txt  \u001b[0m\u001b[01;34mCMakeFiles\u001b[0m/  Makefile  cmake_install.cmake\n",
      "/home/yuma/PycharmProjects/student_cup/notebooks/LightGBM/build/LightGBM/build/LightGBM/build/LightGBM/python-package\n",
      "/home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "running install\n",
      "INFO:LightGBM:Starting to compile the library.\n",
      "INFO:LightGBM:Starting to compile with CMake.\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/lightgbm\n",
      "copying lightgbm/__init__.py -> build/lib/lightgbm\n",
      "copying lightgbm/plotting.py -> build/lib/lightgbm\n",
      "copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
      "copying lightgbm/engine.py -> build/lib/lightgbm\n",
      "copying lightgbm/compat.py -> build/lib/lightgbm\n",
      "copying lightgbm/libpath.py -> build/lib/lightgbm\n",
      "copying lightgbm/basic.py -> build/lib/lightgbm\n",
      "copying lightgbm/callback.py -> build/lib/lightgbm\n",
      "running egg_info\n",
      "creating lightgbm.egg-info\n",
      "writing lightgbm.egg-info/PKG-INFO\n",
      "writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
      "writing requirements to lightgbm.egg-info/requires.txt\n",
      "writing top-level names to lightgbm.egg-info/top_level.txt\n",
      "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
      "reading manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "no previously-included directories found matching 'build'\n",
      "warning: no files found matching '*.txt'\n",
      "warning: no files found matching '*.so' under directory 'lightgbm'\n",
      "warning: no files found matching '*.dll' under directory 'compile/Release'\n",
      "warning: no files found matching '*' under directory 'compile/compute'\n",
      "warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
      "warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
      "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
      "copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
      "running install_lib\n",
      "creating /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/__init__.py -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/plotting.py -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/sklearn.py -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/engine.py -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/compat.py -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/VERSION.txt -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/libpath.py -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/basic.py -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "copying build/lib/lightgbm/callback.py -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "INFO:LightGBM:Installing lib_lightgbm from: ['compile/lib_lightgbm.so']\n",
      "copying compile/lib_lightgbm.so -> /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm\n",
      "byte-compiling /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm/__init__.py to __init__.cpython-38.pyc\n",
      "byte-compiling /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm/plotting.py to plotting.cpython-38.pyc\n",
      "byte-compiling /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm/sklearn.py to sklearn.cpython-38.pyc\n",
      "byte-compiling /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm/engine.py to engine.cpython-38.pyc\n",
      "byte-compiling /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm/compat.py to compat.cpython-38.pyc\n",
      "byte-compiling /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm/libpath.py to libpath.cpython-38.pyc\n",
      "byte-compiling /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm/basic.py to basic.cpython-38.pyc\n",
      "byte-compiling /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm/callback.py to callback.cpython-38.pyc\n",
      "running install_egg_info\n",
      "Copying lightgbm.egg-info to /home/yuma/anaconda3/envs/student_cup/lib/python3.8/site-packages/lightgbm-3.0.0-py3.8.egg-info\n",
      "running install_scripts\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "%cd LightGBM\n",
    "!mkdir build\n",
    "%cd build\n",
    "!cmake ../../LightGBM\n",
    "%ls\n",
    "%cd ../python-package\n",
    "!python3 setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-base-uncased'\n",
    "CASED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>jobflag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>Independently develops forms and related mater...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>Work may include both office and field activit...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>Reporting network operational status by gather...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Customize and manage the ERP System to meet bu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>Maintenance and development of new application...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>Recognize and understand use of design pattern...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>Create and maintain interactive visualizations...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>Design and build integrations with third party...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>Assign and manage project tasks, ensuring task...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>Maintains expert knowledge of ICH/GCP Guidelin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2637 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description  jobflag\n",
       "2547  Independently develops forms and related mater...        4\n",
       "2782  Work may include both office and field activit...        3\n",
       "743   Reporting network operational status by gather...        3\n",
       "62    Customize and manage the ERP System to meet bu...        4\n",
       "2228  Maintenance and development of new application...        3\n",
       "...                                                 ...      ...\n",
       "917   Recognize and understand use of design pattern...        3\n",
       "2825  Create and maintain interactive visualizations...        1\n",
       "2274  Design and build integrations with third party...        3\n",
       "2662  Assign and manage project tasks, ensuring task...        3\n",
       "1274  Maintains expert knowledge of ICH/GCP Guidelin...        1\n",
       "\n",
       "[2637 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tag(row):\n",
    "    \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n",
    "    to_be_inserted = sorted([\n",
    "        # (row[\"description\"], \" [D] \"),\n",
    "        (row[\"jobflag\"], \" [T] \"),\n",
    "        # (row[\"Pronoun-offset\"], \" [P] \")\n",
    "    ], key=lambda x: x[0], reverse=True)\n",
    "    text = row[\"description\"]\n",
    "    for offset, tag in to_be_inserted:\n",
    "        text = text[:offset] + tag + text[offset:]\n",
    "    # print(text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n",
    "    entries = {}\n",
    "    final_tokens = []\n",
    "    for token in tokenizer.tokenize(text):\n",
    "        if token in (\"[T]\"):\n",
    "            entries[token] = len(final_tokens)\n",
    "            continue\n",
    "        final_tokens.append(token)\n",
    "    \n",
    "    return final_tokens, (entries[\"[T]\"])\n",
    "\n",
    "class GAPDataset(Dataset):\n",
    "    \"\"\"Custom GAP Dataset class\"\"\"\n",
    "    def __init__(self, df, tokenizer, labeled=True):\n",
    "        self.labeled = labeled\n",
    "        if labeled:\n",
    "            # tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n",
    "            # tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n",
    "            self.y = df[\"jobflag\"].values # tmp.values.astype(\"bool\")\n",
    "        # Extracts the tokens and offsets(positions of A, B, and P)\n",
    "        self.offsets, self.tokens = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            text = insert_tag(row)\n",
    "            tokens, offsets = tokenize(text, tokenizer)\n",
    "            self.offsets.append(offsets)\n",
    "            self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
    "                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            return self.tokens[idx], self.offsets[idx], self.y[idx]\n",
    "        return self.tokens[idx], self.offsets[idx], None\n",
    "    \n",
    "def collate_examples(batch, truncate_len=500):\n",
    "    \"\"\"Batch preparation.\n",
    "    \n",
    "    1. Pad the sequences\n",
    "    2. Transform the target.\n",
    "    \"\"\"\n",
    "    transposed = list(zip(*batch))\n",
    "    max_len = min(\n",
    "        max((len(x) for x in transposed[0])),\n",
    "        truncate_len\n",
    "    )\n",
    "    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "    for i, row in enumerate(transposed[0]):\n",
    "        row = np.array(row[:truncate_len])\n",
    "        tokens[i, :len(row)] = row\n",
    "    token_tensor = torch.from_numpy(tokens)\n",
    "    # Offsets\n",
    "    offsets = torch.stack([\n",
    "        torch.LongTensor(x) for x in transposed[1]\n",
    "    ], dim=0) + 1 # Account for the [CLS] token\n",
    "    # Labels\n",
    "    if len(transposed) == 2:\n",
    "        return token_tensor, offsets, None\n",
    "    one_hot_labels = torch.stack([\n",
    "        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n",
    "    ], dim=0)\n",
    "    _, labels = one_hot_labels.max(dim=1)\n",
    "    return token_tensor, offsets, labels\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"The MLP submodule\"\"\"\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_size = bert_hidden_size\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_hidden_size * 3),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(bert_hidden_size * 3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                print(\"Initing batchnorm\")\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    print(\"Initing linear with weight normalization\")\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    print(\"Initing linear\")\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, bert_outputs, offsets):\n",
    "        assert bert_outputs.size(2) == self.bert_hidden_size\n",
    "        extracted_outputs = bert_outputs.gather(\n",
    "            1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2))\n",
    "        ).view(bert_outputs.size(0), -1)\n",
    "        return self.fc(extracted_outputs)\n",
    "\n",
    "    \n",
    "class GAPModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self, bert_model: str, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
    "            self.bert_hidden_size = 768\n",
    "        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
    "            self.bert_hidden_size = 1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported BERT model.\")\n",
    "        self.bert = BertModel.from_pretrained(bert_model).to(device)\n",
    "        self.head = Head(self.bert_hidden_size).to(device)\n",
    "    \n",
    "    def forward(self, token_tensor, offsets):\n",
    "        token_tensor = token_tensor.to(self.device)\n",
    "        bert_outputs, _ =  self.bert(\n",
    "            token_tensor, attention_mask=(token_tensor > 0).long(), \n",
    "            token_type_ids=None, output_all_encoded_layers=False)\n",
    "        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n",
    "        return head_outputs            \n",
    "\n",
    "    \n",
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "            \n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n",
    "    \n",
    "    \n",
    "class GAPBot(BaseBot):\n",
    "    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n",
    "        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n",
    "        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n",
    "        device=\"cuda:0\", use_tensorboard=False):\n",
    "        super().__init__(\n",
    "            model, train_loader, val_loader, \n",
    "            optimizer=optimizer, clip_grad=clip_grad,\n",
    "            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n",
    "            batch_idx=batch_idx, echo=echo,\n",
    "            device=device, use_tensorboard=use_tensorboard\n",
    "        )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.loss_format = \"%.6f\"\n",
    "        \n",
    "    def extract_prediction(self, tensor):\n",
    "        return tensor\n",
    "    \n",
    "    def snapshot(self):\n",
    "        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n",
    "        loss = self.eval(self.val_loader)\n",
    "        loss_str = self.loss_format % loss\n",
    "        self.logger.info(\"Snapshot loss %s\", loss_str)\n",
    "        self.logger.tb_scalars(\n",
    "            \"losses\", {\"val\": loss},  self.step)\n",
    "        target_path = (\n",
    "            self.checkpoint_dir / \"best.pth\")        \n",
    "        if not self.best_performers or (self.best_performers[0][0] > loss):\n",
    "            torch.save(self.model.state_dict(), target_path)\n",
    "            self.best_performers = [(loss, target_path, self.step)]\n",
    "        self.logger.info(\"Saving checkpoint %s...\", target_path)\n",
    "        assert Path(target_path).exists()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\").iloc[:, 1:]\n",
    "\n",
    "test = pd.read_csv(\"../data/test.csv\").iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(df, df[\"jobflag\"], \n",
    "                                                            random_state=2018, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.\n",
      "100%|██████████| 231508/231508 [00:00<00:00, 348729.99B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    BERT_MODEL,\n",
    "    do_lower_case=CASED,\n",
    "    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n",
    ")\n",
    "# These tokens are not actually used, so we can assign arbitrary values.\n",
    "tokenizer.vocab[\"[A]\"] = -1\n",
    "tokenizer.vocab[\"[B]\"] = -1\n",
    "tokenizer.vocab[\"[P]\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inde [T] pendently develops forms and related materials that meet regulatory and legal requirements.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[T]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9c36122d4806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAPDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAPDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAPDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_loader = DataLoader(\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f1c19e1c24ee>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, tokenizer, labeled)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minsert_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
      "\u001b[0;32m<ipython-input-29-f1c19e1c24ee>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, tokenizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mfinal_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"[T]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGAPDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[T]'"
     ]
    }
   ],
   "source": [
    "train_ds = GAPDataset(train_x, tokenizer)\n",
    "val_ds = GAPDataset(val_x, tokenizer)\n",
    "test_ds = GAPDataset(test, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=20,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
